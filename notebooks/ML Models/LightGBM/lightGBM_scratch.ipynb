{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc,matthews_corrcoef,f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time \n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../../data/processed/train_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "file_path2= \"../../../data/processed/test_data.csv\"\n",
    "df2 = pd.read_csv(file_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to bin continuous data\n",
    "def bin_data(X, n_bins=255):\n",
    "    binned_X = np.zeros_like(X, dtype=np.int32)\n",
    "    bin_edges = []\n",
    "\n",
    "    for feature in range(X.shape[1]):\n",
    "        # Compute bin edges for the current feature\n",
    "        edges = np.linspace(np.min(X[:, feature]), np.max(X[:, feature]), n_bins + 1)\n",
    "        bin_edges.append(edges)\n",
    "\n",
    "        # Assign each value to a bin\n",
    "        binned_X[:, feature] = np.digitize(X[:, feature], edges) - 1  # Subtract 1 to make bins 0-indexed\n",
    "\n",
    "    return binned_X, bin_edges\n",
    "\n",
    "# Compute residuals and Hessians\n",
    "def compute_residuals_and_hessians(y_true, y_pred):\n",
    "    residuals = y_pred - y_true\n",
    "    hessians = y_pred * (1 - y_pred)  # Second derivative\n",
    "    return residuals, hessians\n",
    "\n",
    "# Find the best split using binned data\n",
    "def find_best_split_binned(binned_X, gradients, hessians, n_bins=255):\n",
    "    def compute_gain_for_feature(feature_idx):\n",
    "        # Count gradients and Hessians for each bin\n",
    "        grad_per_bin = np.zeros(n_bins)\n",
    "        hess_per_bin = np.zeros(n_bins)\n",
    "\n",
    "        for bin_idx in range(n_bins):\n",
    "            mask = binned_X[:, feature_idx] == bin_idx\n",
    "            grad_per_bin[bin_idx] = gradients[mask].sum()\n",
    "            hess_per_bin[bin_idx] = hessians[mask].sum()\n",
    "\n",
    "        # Compute cumulative sums to evaluate all possible splits\n",
    "        cum_grad = np.cumsum(grad_per_bin)\n",
    "        cum_hess = np.cumsum(hess_per_bin)\n",
    "\n",
    "        total_grad = cum_grad[-1]\n",
    "        total_hess = cum_hess[-1]\n",
    "\n",
    "        # Gain for each split (excluding the last bin)\n",
    "        gains = (cum_grad[:-1] ** 2 / (cum_hess[:-1] + 1e-10)) + \\\n",
    "                ((total_grad - cum_grad[:-1]) ** 2 / (total_hess - cum_hess[:-1] + 1e-10))\n",
    "\n",
    "        best_bin = np.argmax(gains)\n",
    "        best_gain = gains[best_bin]\n",
    "\n",
    "        return best_gain, feature_idx, best_bin\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(compute_gain_for_feature)(feature) for feature in range(binned_X.shape[1])\n",
    "    )\n",
    "    best_gain, best_feature, best_bin = max(results, key=lambda x: x[0])\n",
    "    return best_feature, best_bin, best_gain\n",
    "\n",
    "# Recursively split the tree\n",
    "def split_node_binned(binned_X, gradients, hessians, depth, max_depth, max_leaves, bin_edges):\n",
    "    if depth >= max_depth or len(binned_X) <= 1:\n",
    "        leaf_value = -gradients.sum() / hessians.sum() if hessians.sum() > 0 else 0\n",
    "        return {\"leaf_value\": leaf_value}\n",
    "\n",
    "    best_feature, best_bin, best_gain = find_best_split_binned(binned_X, gradients, hessians)\n",
    "    if best_gain <= 0:\n",
    "        leaf_value = -gradients.sum() / hessians.sum() if hessians.sum() > 0 else 0\n",
    "        return {\"leaf_value\": leaf_value}\n",
    "\n",
    "    # Split the data\n",
    "    left_mask = binned_X[:, best_feature] <= best_bin\n",
    "    right_mask = ~left_mask\n",
    "\n",
    "    return {\n",
    "        \"feature\": best_feature,\n",
    "        \"split\": bin_edges[best_feature][min(best_bin + 1, len(bin_edges[best_feature]) - 1)],  # Fix index out of bounds\n",
    "        \"left\": split_node_binned(binned_X[left_mask], gradients[left_mask], hessians[left_mask], \n",
    "                                  depth + 1, max_depth, max_leaves, bin_edges),\n",
    "        \"right\": split_node_binned(binned_X[right_mask], gradients[right_mask], hessians[right_mask], \n",
    "                                   depth + 1, max_depth, max_leaves, bin_edges),\n",
    "    }\n",
    "\n",
    "# Train the LightGBM model\n",
    "def train_lightgbm_with_bins(X, y, learning_rate=0.1, num_trees=10, max_depth=3, max_leaves=31, n_bins=255):\n",
    "    start = time.time()\n",
    "    \n",
    "    binned_X, bin_edges = bin_data(X, n_bins=n_bins)\n",
    "    initial_prediction = np.log(np.mean(y) / (1 - np.mean(y)))\n",
    "    y_pred = np.full_like(y, initial_prediction, dtype=float)\n",
    "\n",
    "    trees = []\n",
    "\n",
    "    for _ in range(num_trees):\n",
    "        gradients, hessians = compute_residuals_and_hessians(y, expit(y_pred))\n",
    "\n",
    "        tree = split_node_binned(binned_X, gradients, hessians, depth=0, max_depth=max_depth, \n",
    "                                 max_leaves=max_leaves, bin_edges=bin_edges)\n",
    "        trees.append(tree)\n",
    "\n",
    "        predictions_update = np.zeros_like(y)\n",
    "        for i in range(len(y)):\n",
    "            node = tree\n",
    "            while \"leaf_value\" not in node:\n",
    "                if X[i, node[\"feature\"]] < node[\"split\"]:\n",
    "                    node = node[\"left\"]\n",
    "                else:\n",
    "                    node = node[\"right\"]\n",
    "            predictions_update[i] = node[\"leaf_value\"]\n",
    "\n",
    "        y_pred += learning_rate * predictions_update\n",
    "\n",
    "    print(f\"Training Time: {time.time() - start:.4f} seconds\")\n",
    "    return trees, initial_prediction, bin_edges\n",
    "\n",
    "# Predict with LightGBM using bins\n",
    "def predict_lightgbm_with_bins(X, trees, initial_prediction):\n",
    "    start = time.time()\n",
    "    \n",
    "    y_pred = np.full(X.shape[0], initial_prediction, dtype=float)\n",
    "    for tree in trees:\n",
    "        predictions_update = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            node = tree\n",
    "            while \"leaf_value\" not in node:\n",
    "                if X[i, node[\"feature\"]] < node[\"split\"]:\n",
    "                    node = node[\"left\"]\n",
    "                else:\n",
    "                    node = node[\"right\"]\n",
    "            predictions_update[i] = node[\"leaf_value\"]\n",
    "\n",
    "        y_pred += predictions_update\n",
    "\n",
    "    print(f\"Testing Time: {time.time() - start:.4f} seconds\")\n",
    "    return expit(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented the lightgbm model with the training and prediction functions and applied it to the train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model without Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train= df.drop(columns=['target']).values\n",
    "y_train= df['target'].values\n",
    "X_test= df2.drop(columns=['target']).values\n",
    "y_test= df2['target'].values\n",
    "\n",
    "start_train_without_fs = time.time()\n",
    "trained_trees_without_fs, initial_prediction_without_fs = train_lightgbm_with_bins(\n",
    "    X_train, y_train, learning_rate=0.1,\n",
    "    num_trees=388,\n",
    "    max_depth=6,\n",
    "    max_leaves=178,\n",
    "    n_bins=138\n",
    ")\n",
    "end_train_without_fs = time.time()\n",
    "training_time_without_fs = end_train_without_fs - start_train_without_fs\n",
    "\n",
    "start_test_without_fs = time.time()\n",
    "y_pred_test_without_fs = predict_lightgbm_with_bins(X_test, trained_trees_without_fs, initial_prediction_without_fs, num_bins=138)\n",
    "end_test_without_fs = time.time()\n",
    "testing_time_without_fs = end_test_without_fs - start_test_without_fs\n",
    "y_pred_test_without_fs = (y_pred_test_without_fs >= 0.5).astype(int)\n",
    "\n",
    "accuracy= accuracy_score(y_test, y_pred_test_without_fs)\n",
    "\n",
    "\n",
    "print(f\"Accuracy sur le jeu de test : {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, (y_pred_test_without_fs >= 0.5).astype(int))\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Classe 0\", \"Classe 1\"], yticklabels=[\"Classe 0\", \"Classe 1\"])\n",
    "plt.title(\"consfusion matrix\")\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_test_without_fs)\n",
    "print(f\"Erreur Quadratique Moyenne (MSE) : {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(y_test, (y_pred_test_without_fs >= 0.5).astype(int))\n",
    "print(\"Rapport de Classification :\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_prob = y_pred_test_without_fs \n",
    "y_pred = (y_pred_test_without_fs >= 0.5).astype(int) \n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)  \n",
    "roc_auc = auc(fpr, tpr)                           \n",
    "\n",
    "# Calcul MCC\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC : {roc_auc:.2%}\")\n",
    "print(f\"MCC : {mcc:.2f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"../../../data/processed/train_filtered.csv\")\n",
    "df_test = pd.read_csv(\"../../../data/processed/test_filtered.csv\")\n",
    "\n",
    "# Prepare training and testing data\n",
    "X_train1 = df_train.drop(columns=['target']).values\n",
    "y_train1 = df_train['target'].values\n",
    "X_test1 = df_test.drop(columns=['target']).values\n",
    "y_test1 = df_test['target'].values\n",
    "\n",
    "# Train LightGBM model\n",
    "start_train_with_fs = time.time()\n",
    "trained_trees_with_fs, initial_prediction_with_fs = train_lightgbm_with_bins(\n",
    "    X_train1, y_train1,\n",
    "    learning_rate=0.1,\n",
    "    num_trees=388,\n",
    "    max_depth=6,\n",
    "    max_leaves=178,\n",
    "    n_bins=138)\n",
    "end_train_with_fs = time.time()\n",
    "training_time_with_fs = end_train_with_fs - start_train_with_fs\n",
    "\n",
    "# Test LightGBM model\n",
    "start_test_with_fs = time.time()\n",
    "y_pred_test_with_fs = predict_lightgbm_with_bins(X_test1, trained_trees_with_fs, initial_prediction_with_fs)\n",
    "end_test_with_fs = time.time()\n",
    "testing_time_with_fs = end_test_with_fs - start_test_with_fs\n",
    "\n",
    "# Convert predicted probabilities to binary class labels\n",
    "y_pred_test_with_fs = (y_pred_test_with_fs >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test1, y_pred_test_with_fs)\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Time: {training_time_with_fs:.4f} seconds\")\n",
    "print(f\"Testing Time: {testing_time_with_fs:.4f} seconds\")\n",
    "print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test1, y_pred_test_with_fs)\n",
    "print(f\"Erreur Quadratique Moyenne (MSE) : {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(y_test, (y_pred_test_with_fs >= 0.5).astype(int))\n",
    "print(\"Rapport de Classification :\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test1, (y_pred_test_with_fs >= 0.5).astype(int))\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Classe 0\", \"Classe 1\"], yticklabels=[\"Classe 0\", \"Classe 1\"])\n",
    "plt.title(\"consfusion matrix\")\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_prob = y_pred_test_with_fs  \n",
    "y_pred = (y_pred_test_with_fs >= 0.5).astype(int) \n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test1, y_prob)  # ROC\n",
    "roc_auc = auc(fpr, tpr)                           # AUC\n",
    "\n",
    "# Calcul MCC\n",
    "mcc = matthews_corrcoef(y_test1, y_pred)\n",
    "\n",
    "print(f\"AUC : {roc_auc:.2%}\")\n",
    "print(f\"MCC : {mcc:.2f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Compute performance metrics\n",
    "# ========================\n",
    "# With Feature Selection\n",
    "class_report_with_fs = classification_report(y_test, y_pred_test_with_fs, output_dict=True)\n",
    "\n",
    "metrics_with_fs = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_test_with_fs),\n",
    "    \"precision\": class_report_with_fs[\"weighted avg\"][\"precision\"],\n",
    "    \"recall\": class_report_with_fs[\"weighted avg\"][\"recall\"],\n",
    "    \"f1_score\": class_report_with_fs[\"weighted avg\"][\"f1-score\"],\n",
    "    \"mcc\": matthews_corrcoef(y_test, y_pred_test_with_fs),\n",
    "    \"training_time\": training_time_with_fs,\n",
    "    \"testing_time\": testing_time_with_fs,\n",
    "}\n",
    "\n",
    "# Without Feature Selection\n",
    "class_report_without_fs = classification_report(y_test, y_pred_test_without_fs, output_dict=True)\n",
    "metrics_without_fs = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_test_without_fs),\n",
    "    \"precision\": class_report_without_fs[\"weighted avg\"][\"precision\"],\n",
    "    \"recall\": class_report_without_fs[\"weighted avg\"][\"recall\"],\n",
    "    \"f1_score\": class_report_without_fs[\"weighted avg\"][\"f1-score\"],\n",
    "    \"mcc\": matthews_corrcoef(y_test, y_pred_test_without_fs),\n",
    "    \"training_time\": training_time_without_fs,\n",
    "    \"testing_time\": testing_time_without_fs,\n",
    "}\n",
    "\n",
    "# ========================\n",
    "# Plotting the comparison\n",
    "# ========================\n",
    "performance_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"mcc\"]\n",
    "time_metrics = [\"training_time\", \"testing_time\"]\n",
    "\n",
    "with_fs_performance = [metrics_with_fs[m] for m in performance_metrics]\n",
    "without_fs_performance = [metrics_without_fs[m] for m in performance_metrics]\n",
    "\n",
    "with_fs_time = [metrics_with_fs[m] for m in time_metrics]\n",
    "without_fs_time = [metrics_without_fs[m] for m in time_metrics]\n",
    "\n",
    "# Set up the bar chart parameters\n",
    "x_perf = np.arange(len(performance_metrics))\n",
    "x_time = np.arange(len(time_metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# ========================\n",
    "# Performance Metrics Plot\n",
    "# ========================\n",
    "rects1 = ax1.bar(x_perf - width / 2, with_fs_performance, width, label=\"With Feature Selection\", color=\"skyblue\")\n",
    "rects2 = ax1.bar(x_perf + width / 2, without_fs_performance, width, label=\"Without Feature Selection\", color=\"salmon\")\n",
    "\n",
    "ax1.set_ylabel(\"Scores\", fontsize=12)\n",
    "ax1.set_title(\"Performance Metrics Comparison\", fontsize=14)\n",
    "ax1.set_xticks(x_perf)\n",
    "ax1.set_xticklabels(performance_metrics, fontsize=12)\n",
    "ax1.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.1), ncol=2, fontsize=12)\n",
    "\n",
    "# Add values to bars\n",
    "for rect in rects1 + rects2:\n",
    "    height = rect.get_height()\n",
    "    ax1.text(\n",
    "        rect.get_x() + rect.get_width() / 2.0,\n",
    "        height + 0.01,\n",
    "        f\"{height:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "# ========================\n",
    "# Time Metrics Plot\n",
    "# ========================\n",
    "rects3 = ax2.bar(x_time - width / 2, with_fs_time, width, label=\"With Feature Selection\", color=\"skyblue\")\n",
    "rects4 = ax2.bar(x_time + width / 2, without_fs_time, width, label=\"Without Feature Selection\", color=\"salmon\")\n",
    "\n",
    "ax2.set_ylabel(\"Time (seconds)\", fontsize=12)\n",
    "ax2.set_title(\"Training and Testing Time Comparison\", fontsize=14)\n",
    "ax2.set_xticks(x_time)\n",
    "ax2.set_xticklabels(time_metrics, fontsize=12)\n",
    "ax2.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.1), ncol=2, fontsize=12)\n",
    "\n",
    "# Add values to bars\n",
    "for rect in rects3 + rects4:\n",
    "    height = rect.get_height()\n",
    "    ax2.text(\n",
    "        rect.get_x() + rect.get_width() / 2.0,\n",
    "        height ,\n",
    "        f\"{height:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
