{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/processed/train_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "file_path2= \"../../data/processed/test_data.csv\"\n",
    "df2 = pd.read_csv(file_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# application du binning for the features to reduce the number of splits\n",
    "def apply_binning(X, num_bins=256):\n",
    "    binned_data = np.zeros_like(X)\n",
    "    #list to store the bin edges for each feature\n",
    "    bin_edges_list = []\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        col = X[:, i]\n",
    "        #min and max values of the feature for sorting the values before binning\n",
    "        bins = np.linspace(col.min(), col.max(), num_bins + 1)\n",
    "        #digitize function to bin the values and subtract 1 to start the bin index from 0\n",
    "        binned_data[:, i] = np.digitize(col, bins) - 1\n",
    "        bin_edges_list.append(bins)\n",
    "    \n",
    "    return binned_data, bin_edges_list\n",
    "\n",
    "# function to initialize the predictions with the log_odds for avoiding the overflow with the values of 0 and 1\n",
    "def initialize_predictions(y):\n",
    "    p = np.mean(y)\n",
    "    log_odds = np.log(p / (1 - p))\n",
    "    return np.full_like(y, 1 / (1 + np.exp(-log_odds)), dtype=np.float32)\n",
    "\n",
    "# function to compute the residuals and hessians for the gradient boosting\n",
    "def compute_residuals_and_hessians(y_true, y_pred):\n",
    "    residuals = y_pred - y_true\n",
    "    #hessians for calculating the second order derivative for knowing the curvature of the loss function\n",
    "    hessians = y_pred * (1 - y_pred)\n",
    "    return residuals, hessians\n",
    "\n",
    "# function to update the predictions after finding the best split\n",
    "def update_predictions(X, best_feature, best_split, gradients, hessians, learning_rate):\n",
    "    predictions_update = np.zeros_like(gradients)\n",
    "    #mask for the samples in the left node\n",
    "    \n",
    "    left_mask = X[:, best_feature] < best_split\n",
    "    #mask for the samples in the right node\n",
    "    right_mask = ~left_mask\n",
    "    #calculating the gradients and hessians for the left and right nodes\n",
    "    left_grad = gradients[left_mask].sum()\n",
    "    left_hess = hessians[left_mask].sum()\n",
    "    right_grad = gradients[right_mask].sum()\n",
    "    right_hess = hessians[right_mask].sum()\n",
    "    \n",
    "    if left_hess > 0:\n",
    "        left_value = -left_grad / left_hess\n",
    "    else:\n",
    "        left_value = 0\n",
    "    \n",
    "    if right_hess > 0:\n",
    "        right_value = -right_grad / right_hess\n",
    "    else:\n",
    "        right_value = 0\n",
    "    \n",
    "    predictions_update[left_mask] = left_value\n",
    "    predictions_update[right_mask] = right_value\n",
    "    \n",
    "    return predictions_update * learning_rate\n",
    "\n",
    "def find_best_split(X, gradients, hessians, num_bins):\n",
    "    # Initialize with negative infinity for assurring taht the first gain will be higher\n",
    "    best_gain = -np.inf\n",
    "    best_feature, best_split = None, None\n",
    "\n",
    "    for feature in range(X.shape[1]): \n",
    "        bin_gradients = np.zeros(num_bins)\n",
    "        bin_hessians = np.zeros(num_bins)\n",
    "        #calculating the gradients and hessians for each bin \n",
    "        for bin_idx in range(num_bins):\n",
    "            #mask for the samples in the bin \n",
    "            mask = (X[:, feature] == bin_idx)  \n",
    "            #summing the gradients and hessians for the samples in the bin for assruing that the bin is not empty\n",
    "            bin_gradients[bin_idx] = gradients[mask].sum()\n",
    "            bin_hessians[bin_idx] = hessians[mask].sum()\n",
    "        #searching for the best split\n",
    "        for split in range(1, num_bins):\n",
    "            #calculating the gain for the split for left and right nodes because the gain is the sum of the gains of the left and right nodes\n",
    "            left_grad = bin_gradients[:split].sum()\n",
    "            left_hess = bin_hessians[:split].sum()\n",
    "            right_grad = bin_gradients[split:].sum()\n",
    "            right_hess = bin_hessians[split:].sum()\n",
    "            #if the hessians are positive we can calculate the gain\n",
    "            if left_hess > 0 and right_hess > 0:\n",
    "                gain = (left_grad ** 2 / left_hess) + (right_grad ** 2 / right_hess)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_split = split\n",
    "\n",
    "    return best_feature, best_split, best_gain\n",
    "# function to train the lightgbm model\n",
    "def train_lightgbm(X, y, num_bins=64, learning_rate=1.2, num_trees=10):\n",
    "    X_binned, bin_edges = apply_binning(X, num_bins=num_bins)\n",
    "    \n",
    "    initial_prediction = np.log(y.mean() / (1 - y.mean()))\n",
    "    y_pred = np.full_like(y, initial_prediction, dtype=float)\n",
    "    \n",
    "    trees = []\n",
    "    \n",
    "    for tree in range(num_trees):\n",
    "        gradients, hessians = compute_residuals_and_hessians(y, 1 / (1 + np.exp(-y_pred)))\n",
    "        \n",
    "        best_feature, best_split, best_gain = find_best_split(X_binned, gradients, hessians, num_bins)\n",
    "        predictions_update = update_predictions(X_binned, best_feature, best_split, gradients, hessians, learning_rate)\n",
    "        y_pred += predictions_update\n",
    "        \n",
    "        tree_structure = {\n",
    "            'feature': best_feature,\n",
    "            'split': best_split,\n",
    "            'left_value': -gradients[X_binned[:, best_feature] < best_split].sum() / hessians[X_binned[:, best_feature] < best_split].sum(),\n",
    "            'right_value': -gradients[X_binned[:, best_feature] >= best_split].sum() / hessians[X_binned[:, best_feature] >= best_split].sum(),\n",
    "        }\n",
    "        trees.append(tree_structure)\n",
    "    \n",
    "    return trees, initial_prediction\n",
    "\n",
    "\n",
    "def predict_lightgbm(X, trees, initial_prediction, num_bins=64):\n",
    "    X_binned, _ = apply_binning(X, num_bins=num_bins)\n",
    "    y_pred = np.full(X.shape[0], initial_prediction, dtype=float)\n",
    "    \n",
    "    for tree in trees:\n",
    "        feature = tree['feature']\n",
    "        split = tree['split']\n",
    "        left_value = tree['left_value']\n",
    "        right_value = tree['right_value']\n",
    "        \n",
    "        mask = X_binned[:, feature] < split\n",
    "        y_pred[mask] += left_value\n",
    "        y_pred[~mask] += right_value\n",
    "    \n",
    "    return 1 / (1 + np.exp(-y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented the lightgbm model with the training and prediction functions and applied it to the train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model with  all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss sur le jeu de test : 1.0564933517785018\n",
      "Accuracy sur le jeu de test : 82.27%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train= df.drop(columns=['target']).values\n",
    "y_train= df['target'].values\n",
    "X_test= df2.drop(columns=['target']).values\n",
    "y_test= df2['target'].values\n",
    "num_bins = 256\n",
    "learning_rate = 0.1\n",
    "num_trees = 100\n",
    "\n",
    "trained_trees, initial_prediction = train_lightgbm(X_train, y_train, num_bins=num_bins, learning_rate=learning_rate, num_trees=num_trees)\n",
    "\n",
    "y_pred_test = predict_lightgbm(X_test, trained_trees, initial_prediction, num_bins=num_bins)\n",
    "\n",
    "loss = log_loss(y_test, y_pred_test)\n",
    "print(f\"Log Loss sur le jeu de test : {loss}\")\n",
    "accuracy = accuracy_score(y_test, (y_pred_test >= 0.5).astype(int))\n",
    "\n",
    "print(f\"Accuracy sur le jeu de test : {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
